{
    "$schema": "https://opencode.ai/config.json",
    "theme": "dark",
    "command": {
        "greet": {
            "template": "echo Hello, {{name}}!",
            "description": "Greets a user",
            "agent": "general",
            "model": "openai/gpt-4",
            "subtask": false
        }
    },
    "watcher": {
        "ignore": [
            "node_modules",
            ".git",
            "dist",
            "build",
            "coverage",
            "*.db",
            "*.rdb",
            "# Logs",
            "logs",
            "*.log"
        ]
    },
    "username": "bthornemail",
    "model": "ollama/qwen3:8b",
    "provider": {
        "ollama": {
            "npm": "@ai-sdk/openai-compatible",
            "name": "Ollama (local)",
            "options": {
                "baseURL": "http://localhost:11434/v1"
            },
            "models": {
                "deepseek-r1:8b": {
                    "name": "DeepSeek R1 8B"
                },
                "qwen3:8b": {
                    "name": "Qwen 3 8B"
                }
            }
        }
    },
    "mcp": {
        "sequentialthinking": {
            "type": "local",
            "command": [
                "npx",
                "-y",
                "@modelcontextprotocol/server-sequential-thinking@latest"
            ],
            "enabled": false
        },
        "redis": {
            "type": "local",
            "command": [
                "npx",
                "-y",
                "@modelcontextprotocol/server-redis"
            ],
            "environment": {
                "REDIS_URL": "redis://127.0.0.1:6379"
            },
            "enabled": false
        },
        "claude": {
            "type": "local",
            "command": [
                "claude",
                "mcp",
                "serve"
            ],
            "environment": {},
            "enabled": false
        },
        "ollama": {
            "type": "local",
            "command": [
                "npx",
                "-y",
                "ollama-mcp-server@latest"
            ],
            "environment": {
                "OLLAMA_BASE_URL": "http://localhost:11434"
            },
            "enabled": false
        },
        "obsidian-mcp": {
            "type": "local",
            "command": [
                "node",
                "/home/main/devops/universal-life-protocol/packages/universal-life-vault/dist/obsidian-mcp.js"
            ],
            "enabled": false
        },
        "identity-mcp": {
            "type": "local",
            "command": [
                "node",
                "/home/main/devops/universal-life-protocol/packages/universal-life-vault/dist/identity-mcp.js"
            ],
            "enabled": false
        }
    },
    "agent": {
        "build": {
            "mode": "primary",
            "model": "ollama/qwen3:8b",
            "prompt": "{file:./prompts/build.txt}",
            "tools": {
                "write": true,
                "edit": true,
                "bash": true
            }
        },
        "plan": {
            "mode": "primary",
            "model": "ollama/qwen3:8b",
            "tools": {
                "write": false,
                "edit": false,
                "bash": false
            }
        },
        "chat": {
            "description": "Engages in a chat with the user",
            "mode": "subagent",
            "model": "anthropic/claude-sonnet-4-20250514",
            "prompt": "You are a chat assistant. Focus on providing helpful and engaging responses.",
            "tools": {
                "write": false,
                "edit": false
            }
        }
    },
    "layout": "auto"
}